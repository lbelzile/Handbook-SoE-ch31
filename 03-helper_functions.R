#' Quantile-quantile plot for parametric double interval truncated data
#'
#' This function maps the observations to a common uniform scale
#' using the probability integral transform based on the fitted model.
#' It then compares this uniform sample to usual plotting positions,
#' but on the exponential scale. To ease comparisons, tolerance intervals
#' which are generated by simulation (see Varty et al., 2021) are added
#' to give a sense of the inherent variability of these.
#' The function takes as argument a matrix \code{pars} which gives draws
#' from the parameter distribution, to reflect the estimation uncertainty.
#' The plotting positions are taken to be the median
#'
#' @param object a fitted object of class \code{elife_par}
#' @param level level of the confidence intervals for the plots
#' @param par a matrix with two columns containing parameter draws for the generalized Pareto
#' @return a ggplot object
qqplot <- function(object,
                   pars,
                   level = 0.95,
                   B = 1000) {
  stopifnot(
    inherits(object, "elife_par"),
    is.matrix(pars),
    ncol(pars) == 2L,
    object$family == "gp",
    object$trunc_type == "doubly interval truncated"
  )
  dat <- object$time
  ltrunc <- object$ltrunc
  rtrunc <- object$rtrunc
  family <- "gp"
  pmod <- function(x, pars) {
    longevity::pgpd(x, scale = pars[1], shape = pars[2])
  }
  ypos_fn <- function(pars) {
    ypos <- pmod(dat, pars)
    F_a2 <-
      ifelse(is.na(ltrunc[, 2]), 0, pmod(ltrunc[, 2], pars = pars))
    F_b2 <-
      ifelse(is.na(rtrunc[, 2]), 0, pmod(rtrunc[, 2], pars = pars))
    F_a1 <-
      ifelse(rtrunc[, 1] == 0, 0, pmod(ltrunc[, 1], pars = pars))
    F_b1 <-
      ifelse(rtrunc[, 1] == 0, 0, pmod(rtrunc[, 1], pars = pars))
    num <- ifelse(dat > rtrunc[, 1],
      ypos - F_a2 + F_b1 - F_a1,
      ypos - F_a1
    )
    return(pmax(0, num) / (F_b2 - F_a2 + F_b1 - F_a1))
  }
  positions <- apply(pars, 1, ypos_fn)
  interv <-
    t(apply(positions, 1, quantile, probs = sort(c((1 - level) / 2, 0.5, 0.5 + level /
      2))))
  # Empirical positions for IID data
  xpos <- qexp(ppoints(length(dat)))
  # Order by median level and map to exponential scale
  interv <- qexp(interv[order(interv[, 2]), ])
  xdat <- data.frame(
    x = xpos,
    ymin = interv[, 1],
    ymax = interv[, 3],
    y = interv[, 2]
  )
  set.seed(2024)
  tolbands <-
    t(apply(
      apply(
        matrix(
          rexp(B * nrow(xdat)),
          nrow = B
        ), 1,
        sort
      ), 1,
      quantile,
      probs = c(0.05, 0.95)
    ))
  mmax <- max(xdat) # max(max(xdat), max(tolbands))
  g <- ggplot(data = xdat) +
    geom_ribbon(
      data = data.frame(
        x = xdat$x,
        ymin = tolbands[, 1],
        ymax = tolbands[, 2]
      ),
      aes(x = x, ymin = ymin, ymax = ymax),
      fill = "gray",
      alpha = 0.25
    ) +
    geom_segment(aes(
      x = x,
      xend = x,
      yend = ymax,
      y = ymin
    )) +
    geom_point(aes(x = x, y = y)) +
    scale_x_continuous(
      limits = c(0, mmax),
      expand = expansion(mult = c(0, 0.05)),
      oob = scales::squish
    ) +
    scale_y_continuous(
      limits = c(0, mmax),
      expand = expansion(mult = c(0, 0.05)),
      oob = scales::squish
    ) +
    labs(y = "standardized observed quantiles", x = "exponential quantiles") +
    theme_classic()
  return(g)
}

cubic_weight <- function(x, x0, bandwidth) {
  W <- function(x) {
    pmax(0, (1 - abs(x)^3)^3)
  }
  W(as.numeric(x - x0) / bandwidth)
}

gaussian_weight <- function(x, x0, bandwidth) {
  W <- function(x) {
    exp(-x^2 / 2)
  }
  W(as.numeric(x - x0) / bandwidth)
}





#' Bivariate peaks over threshold logistic model
#'
#' The function fits a multivariate model
#'
#' @param dat 3D array of observations of dimension \code{n} by \code{D} by 2, with observations (rows), variables (columns) and censoring intervals (depth)
#' @param event matrix of dimension \code{n} by \code{D} with event code, either right censored (0), observed (1), left censored (2) or interval censored (3).
#' @param margthresh \code{D} vector of marginal thresholds above which observations are treated as generalized Pareto
#' @param depthresh probability level defining multivariate exceedances
#' @param margpcens probability level below which multivariate exceedances are left-censored
#' @param scale \code{D} vector of scale parameters for the generalized Pareto margins
#' @param shape \code{D} vector of shape parameters for the generalized Pareto margins
#' @param par scalar, dependence parameter \eqn{\alpha} for the logistic model, must be strictly positive
#' @param method string; either \code{mev} for max-stable copula or \code{mgp} for multivariate generalized Pareto distribution
#' @param Ntot total number of observations, including non-exceedances. If \code{NULL}, taken as the number of rows of \code{dat}
#' @param ecdf1 empirical distribution function of variable 1
#' @param ecdf2 empirical distribution function of variable 2
#' @return value of the log likelihood
bvpot_log <- function(dat,
                      event,
                      depthresh,
                      margthresh,
                      margpcens,
                      scale,
                      shape,
                      par,
                      mdist = c("mev", "mgp"),
                      Ntot = NULL,
                      ecdf1 = NULL,
                      ecdf2 = NULL){
  D <- 2L
  # mdist <- match.arg(mdist)
  mdist <- "mev"
  stopifnot(length(dim(dat)) == 3L)
  n <- dim(dat)[1]
  if(is.null(Ntot)){
    Ntot <- n
  }
  # strip names and ensure correct length
  depthresh <- as.numeric(depthresh)
  margpcens <- rep(margpcens, length.out = D)
  margthresh <- rep(as.numeric(margthresh), length.out = D)
  scale <- rep(as.numeric(scale), length.out = D)
  shape <- rep(as.numeric(shape), length.out = D)
  stopifnot(depthresh >= 0, depthresh < 1,
            all(margpcens >= 0), all(margpcens < 1))
  if(is.null(ecdf1)){
    ecdf1 <- longevity::np_elife(
      time = dat[,1,1],
      time2 = dat[,1,2],
      event = event[,1],
      type = "interval2")$cdf
  } else{
    stopifnot(isTRUE(inherits(ecdf1, "ecdf")))
  }
  if(is.null(ecdf2)){
    ecdf2 <- longevity::np_elife(
      time = dat[,2,1],
      time2 = dat[,2,2],
      event = event[,2],
      type = "interval2")$cdf
  } else{
    stopifnot(isTRUE(inherits(ecdf2, "ecdf")))
  }
  ecdf <- list(ecdf1, ecdf2)
  # Get probability of exceeding the threshold
  lambdau <- 1 - c(ecdf1(margthresh[1]), ecdf2(margthresh[2]))
  ljac <- 0 # log jacobian
  # Marginal inferential censoring probability level
  if(isTRUE(any(margpcens < lambdau))){
    warning("Invalid marginal quantile level")
    margpcens <- pmin(margpcens, 1-lambdau)
  }
  tdat <- dat
  for(i in 1:2){ # iterate over dimension
    for(j in 1:2){ # iterate over data intervals
      # Map data to uniform
      tdat[,i,j] <- ifelse(tdat[,i,j] > margthresh[i],
                       (1-lambdau[i]) + lambdau[i]*mev::pgp(
                         q = tdat[,i,j], loc = margthresh[i], scale = scale[i], shape = shape[i]),
                       ecdf[[i]](tdat[,i,j]))
    }
    # Make sure data are interval format
    tdat[event[, i] == 2, i, 1] <- 0 # left censored
    tdat[event[, i] == 0, i, 2] <- 1 # right censored
    tdat[event[, i] == 1, i, 1] <- tdat[event[, i] == 1, i, 2]
    # Inferential censoring
    infCens <- which(tdat[,i,2] < margpcens[i])
    # Replace observation by lower bound
    if(length(infCens) > 0){
      tdat[infCens,i,1] <- 0
      tdat[infCens,i,2] <- margpcens[i]
      event[infCens,i] <- 2L
    }
    # Extract exceedances for fully observed observations
     exci <- which(tdat[,i,1] > (1-lambdau[i]) & event[,i] == 1L)
     # If any, compute log jacobian
     if(length(exci) > 0){
      logti <- -log1p(pmax(-1,shape[i]*(dat[exci,i,1] - margthresh[i])/scale[i]))/ shape[i]
      if(mdist == "mev"){
        zi <- -1/log(1 - lambdau[i]*exp(logti))
        zi[is.infinite(zi)] <- Inf
        ljac <- ljac + length(exci) * (log(lambdau[i]) - log(scale[i])) +
          sum(2*log(zi) + 1/zi + (1 + shape[i])*logti)
      } else if(mdist == "mgp"){
        ljac <- ljac + (shape[i]-1)*sum(logti) - length(exci)*(log(lambdau[i]) + log(scale[i]))
      }
     }
  }
  # Technically, we want to make sure that observations fall inside the risk region
  isExc <- apply(tdat[,,1], 1, function(x){isTRUE(any(x > depthresh))})
  # Shed empty lines that do not contribute to exceedances (all below the threshold)
  tdat <- tdat[isExc,,]
  event <- event[isExc,,drop = FALSE]
  Nbelow <- Ntot - dim(tdat)[1] # Compute number below threshold
  if(mdist == "mev"){
  frech <- mev::qgev(tdat, 1,1,1)
  logist_mev <- bvpot_logist_dep(tdat = frech,
                                 thresh = -1/log(depthresh),
                                 event = event,
                                 par = par,
                                 Ntot = Ntot)
     return(ljac +  sum(logist_mev$loglik) - Nbelow * logist_mev$log_exponentmeasure)
  } else if(mdist == "mgp"){
    stop("Currently not implemented")
    # logist_mgp <- mgp_elife(tdat = 1/(1-tdat),
    #                         thresh = 1/lambdau,
    #                         event = event,
    #                         par = par)
    # return(as.numeric(logist_mgp) + ljac)
  }
}



#' Exponent measure of the logistic distribution
#'
#' @param x vector or matrix of observations
#' @param alpha parameter vector \eqn{\alpha \in (0,1)}
#' @param log logical; if \code{TRUE}, return log of exponent measure
#' @return value of the (log) exponent measure
V_logist <- function(x, alpha, log = TRUE) {
  if (is.vector(x)) {
    lV <- alpha * log(sum(x^(-1 / alpha)))
  } else if(is.matrix(x)) {
    lV <- alpha * log(rowSums(x^(-1 / alpha)))
  } else{
    stop("Data format not supported.")
  }
  if (log) {
    return(lV)
  } else {
    return(exp(lV))
  }
}


# Falling factorial, with special treatment for integer alpha
lfalfact <- function(x, s) {
  stopifnot(is.numeric(x), x >= 0, x < 1, length(x) == 1L)
  if (x == 1) {
    return(rep(0, length(s)))
  } else {
    return(lgamma(x + 1) - lgamma(x - s + 1))
  }
}
# Log of the kth derivative of the exponent measure
ldV_logist <- function(x, censored, alpha, lV) {
  stopifnot(is.matrix(x))
  if(missing(censored)){
    # Every observation above the threshold
    censored <- matrix(FALSE, nrow = nrow(x), ncol = ncol(x))
  } else {
    stopifnot(is.matrix(censored),
              dim(x) == dim(censored))
  }
  D <- ncol(x)
  numAbovePerRow <- ncol(x) - rowSums(censored)
  if (missing(lV)) {
    lV <- V_logist(x, alpha = alpha, log = TRUE)
  }
  # Precompute falling factorial to avoid having to repeat this
  # 0 in the first entry is a placeholder for non-exceedances
  falf <- c(log(alpha), sapply(2:D, function(s) {
    lfalfact(alpha, s)}))
  # Log derivative of V for the whole sample
  -numAbovePerRow * log(alpha) + falf[numAbovePerRow] -
    (1 / alpha + 1) * rowSums(log(x) * (1 - censored), na.rm = TRUE) +
    (alpha - numAbovePerRow) / alpha * lV
}

#' Bivariate max-stable likelihood with censoring
#'
#' @param tdat transformed data array with unit Frechet margins
#' @param par dependence parameter for the logistic distribution
#' @param event \code{n} by 2 matrix of event codes, either 0 for right censoring, 1 for fully observed data, 2 for left censoring and 3 for interval censoring.
#' @param thresh vector of thresholds for the risk region, in unit Frechet margins
#' @param Ntot integer number of exceedances, greater or equal to the number in tdat.
bvpot_logist_dep <- function(tdat,
                     par,
                     event,
                     thresh,
                     Ntot = NULL){

  # Marginal transformation
  # The Ledford and Tawn paper considers transformation
  # of generalized Pareto margins and maps data to unit
  #
  # If we consider inferential censoring, only 'exceedances'
  # above functional threshold contributes to the Jacobian
  # for the marginal parameters
  #
  # Frechet scale, then uses MEV distribution
  # It also includes a contribution for the number below
  # the threshold
  tu <- thresh # map threshold to unit frechet
  alpha <- as.numeric(unlist(par)) # extract parameter
  if (alpha > 1) {
    alpha <- 1 / alpha
  }
  if (alpha < 0) {
    stop("Invalid \"par\" for \"log\" model.")
  }
  stopifnot(alpha > 0, length(alpha) == 1L)
  model <- "log"
  n <- dim(tdat)[1] # Number of observations
  D <- 2 # dimension of the multivariate problem
  if(is.null(Ntot)){
    Ntot <- n
  }
  # Distribution function of the logistic distribution
  pbev_logist <- function(x, alpha, log = FALSE){
    logF <- -V_logist(x = x, alpha = alpha, log = FALSE)
    if(log){
      return(logF)
    } else{
      return(exp(logF))
    }
  }
  # Derivative of bivariate logistic with respect to arguments in censored
  d1bev_logist <- function(x, censored, alpha, log = FALSE){
    if(!is.matrix(x)){
      x <- matrix(x, ncol = 2)
    }
    lV <- V_logist(x = x, alpha = alpha, log = TRUE)
    dV <- ldV_logist(x = x, censored = censored, lV = lV, alpha = alpha)
    logd <- dV - exp(lV)
    if(log){
      return(logd)
    } else{
      return(exp(logd))
    }
  }
  # Density function of bivariate extreme value logistic model
  dbev_logist <- function(x, alpha, log = FALSE){
    if(!is.matrix(x)){
      x <- as.matrix(x, ncol = 2)
    }
    n <- nrow(x)
    lV <- V_logist(x = x, alpha = alpha, log = TRUE)
    a <- ldV_logist(x = x, censored = cbind(rep(TRUE, n), rep(FALSE, n)),
                    alpha = alpha, lV = lV) +
      ldV_logist(x = x, censored = cbind(rep(FALSE, n), rep(TRUE, n)),
                 alpha = alpha, lV = lV)
    b <- ldV_logist(x = x, alpha = alpha, lV = lV)
    #(V1V2-V12), but V12 is negative and the function returns abs(log(V12))
    logd <- -exp(lV) + copula:::lsum(lx = rbind(a,b))
    if(log){
      return(logd)
    } else{
      return(exp(logd))
    }
  }

  # Likelihood contributions - MORE COMPLICATED WHEN D>2!
  # Case 0: both left-censored
  lVu <- pbev_logist(x = tu, alpha = alpha)
  # Case 1: both observed
  censored <- event != 1
  eventsOne <- rowSums(event == 1)
  full <- eventsOne == D
  # Case 2: only one observed
  partcens <- eventsOne == 1
  # Case 3: both censored, but observation is exceedance above u
  fullcens <- eventsOne == 0
  loglik <- rep(0,3)
  if (sum(full) > 0) {
    # log density for fully observed
    loglik[1] <- sum(dbev_logist(x = tdat[full, , 1], alpha = alpha, log = TRUE))
  }
  if (sum(partcens) > 0) {
    lV1a <- d1bev_logist(
      x = tdat[partcens, , 2],
      alpha = alpha,
      censored = censored[partcens, , drop = FALSE], log = TRUE)
    lV1b <- d1bev_logist(
        x = tdat[partcens, , 1],
        alpha = alpha,
        censored = censored[partcens, , drop = FALSE], log = TRUE)
    nonzero <- is.finite(lV1b)
    lV1 <- numeric(length = length(lV1b))
    if(isTRUE(any(nonzero))){
    lV1[nonzero] <- copula:::lssum(lxabs = rbind(lV1a, lV1b)[,nonzero],
                   signs = c(1,-1),
                   strict = FALSE)
    }
    lV1[!nonzero] <- lV1a[!nonzero]
    # Use copula:::lssum() for stability, keep things on log scale
    loglik[2] <- sum(lV1)
  }
  if (sum(fullcens) > 0) {
    loglik[3] <- sum(log(
      pbev_logist(tdat[fullcens, , 2], alpha = alpha) -
      pbev_logist(cbind(tdat[fullcens, 1, 2],
                            tdat[fullcens, 2, 1]),
                  alpha = alpha) -
      pbev_logist(cbind(tdat[fullcens, 1, 1],
                            tdat[fullcens, 2, 2]),
                  alpha = alpha) +
      pbev_logist(tdat[fullcens, , 1], alpha = alpha)))
  }
  list(loglik = loglik, log_exponentmeasure = lVu)

}
